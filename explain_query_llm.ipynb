{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ChatGPT to explain SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI key as an environment variable\n",
    "import os\n",
    "os.environ['OPENAI_KEY'] = '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'\n",
    "open_ai_key = os.environ.get('OPENAI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evadb\n",
    "cursor = evadb.connect().cursor()\n",
    "\n",
    "# Run ChatGPT over the Text Summary extracted by Whisper\n",
    "chatgpt_udf = \"\"\"\n",
    "    SELECT ChatGPT('Please explain this query', \"SELECT * FROM students\");\n",
    "\"\"\"\n",
    "response = cursor.query(chatgpt_udf).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    chatgpt.response\n",
      "0  The query \"SELECT * FROM students\" is used to ...\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chatgpt.response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To explain the query plan generated for the gi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    chatgpt.response\n",
       "0  To explain the query plan generated for the gi..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run ChatGPT over the Text Summary extracted by Whisper\n",
    "chatgpt_udf = \"\"\"\n",
    "    SELECT ChatGPT('Please explain the query plan generated for this query using MySQL', \"SELECT * FROM students\");\n",
    "\"\"\"\n",
    "cursor.query(chatgpt_udf).df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPT4All to explain SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evadb\n",
    "cursor = evadb.connect().cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/krishnathan/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[93204]: Class GGMLMetalClass is implemented in both /Users/krishnathan/Documents/College/Fall 23/CS 6422/Project/evadb-project/evadb_env/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x159068228) and /Users/krishnathan/Documents/College/Fall 23/CS 6422/Project/evadb-project/evadb_env/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x159180228). One of the two will be used. Which one is undefined.\n",
      "llama.cpp: using Metal\n",
      "llama.cpp: loading model from /Users/krishnathan/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 3200\n",
      "llama_model_load_internal: n_mult     = 240\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 26\n",
      "llama_model_load_internal: n_rot      = 100\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 8640\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 3B\n",
      "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
      "llama_model_load_internal: mem required  = 2194.73 MB (+  650.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  650.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/krishnathan/Documents/College/Fall 23/CS 6422/Project/evadb-project/evadb_env/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x1044f36f0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x15708c330\n",
      "ggml_metal_init: loaded kernel_mul                            0x15708cbe0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x15708d140\n",
      "ggml_metal_init: loaded kernel_scale                          0x15708d9a0\n",
      "ggml_metal_init: loaded kernel_silu                           0x15708e2c0\n",
      "ggml_metal_init: loaded kernel_relu                           0x15708eb00\n",
      "ggml_metal_init: loaded kernel_gelu                           0x15708f340\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x157090d20\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x15708fe50\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x104378a00\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x104316c70\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x1043971a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x157090680\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x157091c50\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x157091fd0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x1044f4bb0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x1044f5a10\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x1044f5ec0\n",
      "ggml_metal_init: loaded kernel_norm                           0x1044f6e50\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x16429c6d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x157092810\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x157093480\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x1044f7370\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x1044f82c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x1570939a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x157094770\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x157094c70\n",
      "ggml_metal_init: loaded kernel_rope                           0x157095f60\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x1044f8dd0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1044f9a20\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x157095780\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x16429d790\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 10922.67 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =    54.93 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  1839.12 MB, ( 1839.58 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     8.17 MB, ( 1847.75 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   652.00 MB, ( 2499.75 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   220.00 MB, ( 2719.75 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   128.00 MB, ( 2847.75 / 10922.67)\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain what SELECT does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and how it works.\n",
      "A SELECT statement is used to retrieve data from one or more tables in a database. It consists of two parts: the\n"
     ]
    }
   ],
   "source": [
    "tokens = model.generate(\"Explain what a SELECT statement does\", max_tokens=30, streaming=True)\n",
    "print(\"\".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "The SELECT statement is used to retrieve data from one or more tables in a database. It requires the use of a table name and optional columns, separated by commas. The FROM clause specifies which table(s) you want to retrieve data\n"
     ]
    }
   ],
   "source": [
    "tokens = list(model.generate(\"Explain what 'SELECT * FROM students' does\", max_tokens=50, streaming=True))\n",
    "print(\"\".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper function to explain general query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_sql(query_string: str, response_length: int) -> str:\n",
    "    tokens = list(model.generate(\"Explain what the SQL query: '{}' does\".format(query_string), max_tokens=response_length, streaming=True))\n",
    "    text = \"\".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "The SQL query 'SELECT * FROM students' is used to retrieve all columns (i.e., all data) from a table named \"students\". The asterisk (*) is a wildcard character that represents all columns in the\n"
     ]
    }
   ],
   "source": [
    "print(explain_sql(\"SELECT * FROM students\", 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that this also works for INSERT and GROUP queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "The given SQL query is used to insert a new row into the table \"students\" with an ID of 1 and a name of \"Krish\". The \"id\" column specifies the unique identifier for each row in the table\n"
     ]
    }
   ],
   "source": [
    "print(explain_sql(\"INSERT INTO students (id, name) VALUES ('1', 'Krish')\", 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". This query will retrieve all the information from the table 'students' where the 'id' column is equal to 1 and the 'name' column is equal to 'Krish'. The results of this query would be a list of students with their respective classes and IDs.\n"
     ]
    }
   ],
   "source": [
    "# print(\n",
    "# explain_sql(\"\"\" \n",
    "# SELECT COUNT(CustomerID), Country\n",
    "# FROM Customers\n",
    "# GROUP BY Country\n",
    "# ORDER BY COUNT(CustomerID) DESC;\"\"\", \n",
    "# 50))\n",
    "\n",
    "print(\n",
    "explain_sql(\"\"\"SELECT * FROM students WHERE id = 1, name = Krish \n",
    "GROUP BY class ORDER BY id\"\"\", \n",
    "100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular model, orca-mini, struggles sometimes with explaining the INSERT and GROUP queries. For example, sometimes it will just return the string \".\". I believe this is because the model is relatively small (1.8 GB) to make it possible to download for local use. However, there are much fewer parameters in the LLM and my laptop has much less compute than is available on ChatGPT. Thus, the model does not always produce the desired response\n",
    "\n",
    "The specific prompt given to the LLM may influence the output significantly. I made sure to specify that the given text is a SQL query in my prompt. I believe this yields more consistent results, although the LLM sometimes does not give an answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
