{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPT4All to explain SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evadb\n",
    "cursor = evadb.connect().cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnathan/Documents/College/Fall 23/CS 6422/Project/evadb-project/evadb_env/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/krishnathan/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[21466]: Class GGMLMetalClass is implemented in both /Users/krishnathan/Documents/College/Fall 23/CS 6422/Project/evadb-project/evadb_env/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x13dd14228) and /Users/krishnathan/Documents/College/Fall 23/CS 6422/Project/evadb-project/evadb_env/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x13dbdc228). One of the two will be used. Which one is undefined.\n",
      "llama.cpp: using Metal\n",
      "llama.cpp: loading model from /Users/krishnathan/.cache/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 3200\n",
      "llama_model_load_internal: n_mult     = 240\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 26\n",
      "llama_model_load_internal: n_rot      = 100\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 8640\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 3B\n",
      "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
      "llama_model_load_internal: mem required  = 2194.73 MB (+  650.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  650.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/krishnathan/Documents/College/Fall 23/CS 6422/Project/evadb-project/evadb_env/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x13e551150\n",
      "ggml_metal_init: loaded kernel_add_row                        0x148196cf0\n",
      "ggml_metal_init: loaded kernel_mul                            0x1481d42c0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x148152d70\n",
      "ggml_metal_init: loaded kernel_scale                          0x107bd90d0\n",
      "ggml_metal_init: loaded kernel_silu                           0x107bc7e80\n",
      "ggml_metal_init: loaded kernel_relu                           0x107bd94e0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x13e552630\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x13e552e00\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x13e5531e0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x13e5537c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x13e554670\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13e554c50\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13e554fa0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13e5562a0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13e556b40\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13e5573b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13e557c80\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x13e558530\n",
      "ggml_metal_init: loaded kernel_norm                           0x13e559500\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13e55a270\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13e55aa10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x13e55b300\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x13e55bc20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x13e55bfb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x13e55cf90\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x13e55d890\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x13e55e190\n",
      "ggml_metal_init: loaded kernel_rope                           0x13e55ebc0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x13e55e510\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x13e55f9b0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x13e5605c0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x13e561070\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 10922.67 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =    54.93 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  1839.12 MB, ( 1839.58 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     8.17 MB, ( 1847.75 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   652.00 MB, ( 2499.75 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   220.00 MB, ( 2719.75 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   128.00 MB, ( 2847.75 / 10922.67)\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain what SELECT does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and how it works.\n",
      "A SELECT statement is used to retrieve data from one or more tables in a database. It consists of several parts, including\n"
     ]
    }
   ],
   "source": [
    "tokens = model.generate(\"Explain what a SELECT statement does\", max_tokens=30, streaming=True)\n",
    "print(\"\".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "The SELECT statement is used to retrieve data from one or more tables in a database. It requires the name of the table(s) you want to select data from and specifies which columns to include in the result set. The query can then\n"
     ]
    }
   ],
   "source": [
    "tokens = list(model.generate(\"Explain what 'SELECT * FROM students' does\", max_tokens=50, streaming=True))\n",
    "print(\"\".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper function to explain general query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_sql(query_string: str, response_length: int) -> str:\n",
    "    tokens = list(model.generate(\"Explain what the SQL query: '{}' does\".format(query_string), max_tokens=response_length, streaming=True))\n",
    "    text = \"\".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "The SQL query 'SELECT * FROM students' is used to retrieve all columns (i.e., all data) from a table named \"students\". The asterisk (*) symbol means that we want to select all columns in the\n"
     ]
    }
   ],
   "source": [
    "print(explain_sql(\"SELECT * FROM students\", 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that this also works for INSERT and GROUP queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "The given SQL query is used to insert a new row into the table \"students\" with an ID of 1 and a name of \"Krish\". The \"VALUES\" keyword specifies which column(s) to insert data\n"
     ]
    }
   ],
   "source": [
    "print(explain_sql(\"INSERT INTO students (id, name) VALUES ('1', 'Krish')\", 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "explain_sql(\"\"\" \n",
    "SELECT COUNT(CustomerID), Country\n",
    "FROM Customers\n",
    "GROUP BY Country\n",
    "ORDER BY COUNT(CustomerID) DESC;\"\"\", \n",
    "50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular model, orca-mini, struggles sometimes with explaining the INSERT and GROUP queries. For example, sometimes it will just return the string \".\". I believe this is because the model is relatively small (1.8 GB) to make it possible to download for local use. However, there are much fewer parameters in the LLM and my laptop has much less compute than is available on ChatGPT. Thus, the model does not always produce the desired response\n",
    "\n",
    "The specific prompt given to the LLM may influence the output significantly. I made sure to specify that the given text is a SQL query in my prompt. I believe this yields more consistent results, although the LLM sometimes does not give an answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
